{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os, shutil, glob, sys, math, cv2, re\n",
    "\n",
    "import albumentations as albu\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "# from torch.utils import load_state_dict_from_url\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/home/Tsung/pathology/data/tcga/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D model\n",
    "class attentionPart(nn.Module):\n",
    "    def __init__(self, d_model = 512):\n",
    "        super().__init__()\n",
    "#         self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
    "        self.transformer_encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
    "\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=6),\n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "            Flatten(),\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):       \n",
    "        attention = self.attention_layer(x)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init 一定要做\n",
    "class other_feature_encoding(nn.Module):\n",
    "    def __init__(self, num_features = 100, d_model = 512):\n",
    "        super().__init__()\n",
    "        self.n_features = num_features\n",
    "        self.wm = nn.Parameter(torch.Tensor(num_features, d_model))\n",
    "        self.wm = nn.init.normal_(self.wm)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, self.n_features, 1)\n",
    "        x = x * self.wm\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D model\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, max_images = 200, d_model = 512, number_other_feature = 200, dropout=0.1, nclass=10):\n",
    "        super().__init__()\n",
    "        self.nclass = nclass\n",
    "        self.CNN = resnetSmall()\n",
    "        self.CNN.fc = nn.Identity()\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_images)\n",
    "        \n",
    "        self.other_feature_encoding = other_feature_encoding(num_features = number_other_feature, d_model = d_model)\n",
    "    \n",
    "        self.attention_layer = attentionPart(d_model)\n",
    "        \n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(max_images + number_other_feature, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),    \n",
    "            nn.Linear(32, nclass)\n",
    "        )\n",
    "        # softmax\n",
    "    def forward(self, x, x2):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        cnn_input = x.view(batch_size * timesteps, C, H, W)\n",
    "        cnn_output = self.CNN(cnn_input)\n",
    "        other_feature = self.other_feature_encoding(x2)\n",
    "        att_input = cnn_output.view(batch_size, timesteps, -1)\n",
    "        att_input = self.pos_encoder(att_input)\n",
    "        # concat two data\n",
    "        att_input = torch.cat([att_input, other_feature], dim = 1)\n",
    "        att_output = self.attention_layer(att_input)\n",
    "        cls = self.classify(att_output)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = classifier(max_images = 10, d_model = 512, number_other_feature=17, nclass = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn(1, 10, 3, 512, 512)\n",
    "# b = torch.randn(1, 17)\n",
    "# out = model(a, b)\n",
    "# out = out.detach().cpu().numpy()\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt2Classy(label):\n",
    "    cut = np.array([100,200])\n",
    "    \n",
    "    _foo = (label < cut)*1\n",
    "    if np.sum(_foo) == 0:\n",
    "        return len(cut)\n",
    "    else:\n",
    "        for _temp, _i in enumerate(_foo):\n",
    "            if _i == 1:\n",
    "                return _temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數量500多，有些沒有images\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/coad_Mutation_Count.txt', delimiter=\"\\t\")\n",
    "cohort_count_dict = {row[1]: row[3] for index,row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./no_RNA_no_onehot_other_feature.pkl', 'rb') as fp:\n",
    "    cohort_other_feature_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using percentile, only choose high and low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_and_low_cohort(all_cohorts = None, cohort_mc_dict = None ):\n",
    "    # all cohort 只有300多人， cohort_mc_dict 有 500 多人\n",
    "    all_counts = np.array(list(cohort_mc_dict.values()))\n",
    "    percentile = np.percentile(all_counts, [25,50,75])\n",
    "    \n",
    "    low_cohort = []\n",
    "    high_cohort = []\n",
    "    for one_cohort in all_cohorts:\n",
    "        if cohort_mc_dict[one_cohort] <= percentile[0]:\n",
    "            low_cohort.append(one_cohort)\n",
    "        elif cohort_mc_dict[one_cohort] >= percentile[2]:\n",
    "            high_cohort.append(one_cohort)\n",
    "    print(\"high-{} : {}\\nlow-{} : {}\".format(percentile[2], len(high_cohort), percentile[0], len(low_cohort)))\n",
    "    return np.array(high_cohort), np.array(low_cohort)\n",
    "high_cohort, low_cohort = high_and_low_cohort(all_cohorts = list(cohort_other_feature_dict.keys()), cohort_mc_dict = cohort_count_dict)\n",
    "\n",
    "using_cohorts = np.concatenate((high_cohort, low_cohort))\n",
    "print(len(using_cohorts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cohorts, valid_cohorts = train_test_split(\n",
    "    using_cohorts, test_size=0.33, random_state=0)\n",
    "print(len(train_cohorts), len(valid_cohorts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense200_folders = os.listdir(os.path.join(DATA_FOLDER, '1000dense200_npy'))\n",
    "train_image_folders = []\n",
    "valid_image_folders = []\n",
    "train_counts = []\n",
    "valid_counts = []\n",
    "\n",
    "for folder_name in dense200_folders:\n",
    "    if not folder_name.startswith('TCGA'):\n",
    "        continue\n",
    "    cohort_name = folder_name[:12]\n",
    "    flag = folder_name[13]\n",
    "    if flag == '1':\n",
    "        continue\n",
    "    if cohort_name in train_cohorts:\n",
    "        train_image_folders.append(folder_name)\n",
    "        if cohort_name in high_cohort:\n",
    "            train_counts.append(1)\n",
    "        elif cohort_name in low_cohort:\n",
    "            train_counts.append(0)\n",
    "    if cohort_name in valid_cohorts:\n",
    "        valid_image_folders.append(folder_name)\n",
    "        if cohort_name in high_cohort:\n",
    "            valid_counts.append(1)\n",
    "        elif cohort_name in low_cohort:\n",
    "            valid_counts.append(0)\n",
    "        \n",
    "train_image_folders = np.array(train_image_folders)\n",
    "valid_image_folders = np.array(valid_image_folders)\n",
    "train_counts = np.array(train_counts)\n",
    "valid_counts = np.array(valid_counts)\n",
    "print(len(train_image_folders), len(valid_image_folders), len(train_counts), len(valid_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using 100,200,300, low, medium, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cohorts, valid_cohorts = train_test_split(\n",
    "    list(cohort_other_feature_dict.keys()), test_size=0.33, random_state=0)\n",
    "print(len(train_cohorts), len(valid_cohorts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense200_folders = os.listdir(os.path.join(DATA_FOLDER, '1000dense200'))\n",
    "train_image_folders = []\n",
    "valid_image_folders = []\n",
    "train_counts = []\n",
    "valid_counts = []\n",
    "\n",
    "for folder_name in dense200_folders:\n",
    "    if not folder_name.startswith('TCGA'):\n",
    "        continue\n",
    "    cohort_name = folder_name[:12]\n",
    "    flag = folder_name[13]\n",
    "    if flag == '1':\n",
    "        continue\n",
    "    if cohort_name in train_cohorts:\n",
    "        train_image_folders.append(folder_name)\n",
    "        train_counts.append(cvt2Classy(cohort_count_dict[cohort_name]))\n",
    "    if cohort_name in valid_cohorts:\n",
    "        valid_image_folders.append(folder_name)\n",
    "        valid_counts.append(cvt2Classy(cohort_count_dict[cohort_name]))\n",
    "        \n",
    "train_image_folders = np.array(train_image_folders)\n",
    "valid_image_folders = np.array(valid_image_folders)\n",
    "train_counts = np.array(train_counts)\n",
    "valid_counts = np.array(valid_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_counts),\n",
    "                                                 train_counts)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(np.count_nonzero(train_counts == i))\n",
    "print('\\n')\n",
    "for i in range(3):\n",
    "    print(np.count_nonzero(valid_counts == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(valid_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p = 0.5),\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_validaugmentation(image_size):\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(height = image_size, width = image_size, always_apply=True),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py, to_tensor     \n",
    "def to0_1(x, **kwargs):\n",
    "    return x/255\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "        albu.Lambda(image=to0_1, mask=to0_1),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    def __init__(self, image_folder_array, label_array, other_feature_dict, image_size = None, augmentation=None, preprocessing=None):\n",
    "        self.image_folder_array = image_folder_array\n",
    "        self.label_array = label_array\n",
    "        self.other_feature_dict = other_feature_dict\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        fp = self.image_folder_array[i]\n",
    "        images = np.load(os.path.join(DATA_FOLDER, '1000dense200_npy', fp))\n",
    "        \n",
    "        for idx, img in enumerate(images):\n",
    "            if self.augmentation:\n",
    "                sample = self.augmentation(image=img)\n",
    "                images[idx] = sample['image']\n",
    "        images = images.transpose(0,3,1,2).astype('float32')\n",
    "            \n",
    "        label = self.label_array[i]\n",
    "        label = label.astype('int64')\n",
    "\n",
    "        other_feature = self.other_feature_dict[fp[:12]].astype('float32')\n",
    "            \n",
    "        return images, other_feature, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr = 1e-4):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_weights_for_balanced_classes(_y):\n",
    "#     nclasses = len(np.unique(_y))\n",
    "#     weight_per_class = {i: 0. for i in range(10)}\n",
    "#     count_per_class = {i: 0. for i in np.unique(_y)}\n",
    "#     for i in _y:\n",
    "#         count_per_class[i] += 1    \n",
    "#     for i in np.unique(_y):\n",
    "#         weight_per_class[i] = len(_y)/float(count_per_class[i])\n",
    "#     weights = [0.] * len(_y)\n",
    "#     for idx, val in enumerate(_y):\n",
    "#         weights[idx] = weight_per_class[val]\n",
    "#     return weights, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, weight_per_class = make_weights_for_balanced_classes(np.log(train_counts).astype('uint8'))\n",
    "# weights = torch.DoubleTensor(weights)\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# _temp = 0\n",
    "# for key, value in weight_per_class.items():\n",
    "#     _temp += value\n",
    "\n",
    "# weight_per_class = [weight_per_class[i]/_temp for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "nclass = 2\n",
    "epoch = 50\n",
    "model_arch = 'resnetSmall'\n",
    "init_lr = 1e-4\n",
    "target = 'cls'\n",
    "\n",
    "nfeature = None\n",
    "for key, value in cohort_other_feature_dict.items():\n",
    "    if nfeature is None:\n",
    "        nfeature = len(value)\n",
    "    if nfeature != len(value):\n",
    "        print(\"Error, feature number error\")\n",
    "print(nfeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    train_image_folders,\n",
    "    train_counts,\n",
    "    cohort_other_feature_dict,\n",
    "    augmentation = get_augmentation()\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_image_folders,\n",
    "    valid_counts,\n",
    "    cohort_other_feature_dict,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=12)\n",
    "    \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(max_images = 200, d_model = 512, number_other_feature=nfeature, nclass = nclass)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "train_loss = 'ce'\n",
    "train_metric = 'acc'\n",
    "\n",
    "loss = utils.metrics.CrossEntropy(weight=class_weights.astype('float32'))\n",
    "metrics = [\n",
    "    utils.metrics.Accuracy(),\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=init_lr),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "class Epoch:\n",
    "\n",
    "    def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.stage_name = stage_name\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "        self._to_device()\n",
    "\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        self.loss.to(self.device)\n",
    "        for metric in self.metrics:\n",
    "            metric.to(self.device)\n",
    "\n",
    "    def _format_logs(self, logs):\n",
    "        str_logs = ['{} - {:.4}'.format(k, v) for k, v in logs.items()]\n",
    "        s = ', '.join(str_logs)\n",
    "        return s\n",
    "\n",
    "    def batch_update(self, x, x2, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, dataloader):\n",
    "\n",
    "        self.on_epoch_start()\n",
    "\n",
    "        logs = {}\n",
    "        loss_meter = utils.meter.AverageValueMeter()\n",
    "        metrics_meters = {metric.__name__: utils.meter.AverageValueMeter() for metric in self.metrics}\n",
    "\n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for x, x2, y in iterator:\n",
    "                x, x2, y = x.to(self.device), x2.to(self.device), y.to(self.device)\n",
    "                loss, y_pred = self.batch_update(x, x2, y)\n",
    "\n",
    "                # update loss logs\n",
    "                loss_value = loss.cpu().detach().numpy()\n",
    "                loss_meter.add(loss_value)\n",
    "                loss_logs = {self.loss.__name__: loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "\n",
    "                # update metrics logs\n",
    "                for metric_fn in self.metrics:\n",
    "                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n",
    "                    metrics_meters[metric_fn.__name__].add(metric_value)\n",
    "                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n",
    "                logs.update(metrics_logs)\n",
    "\n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "\n",
    "        return logs\n",
    "\n",
    "\n",
    "class TrainEpoch(Epoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, x, x2, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model.forward(x, x2)\n",
    "        loss = self.loss(prediction, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss, prediction\n",
    "\n",
    "\n",
    "class ValidEpoch(Epoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='valid',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def batch_update(self, x, x2, y):\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model.forward(x, x2)\n",
    "            loss = self.loss(prediction, y)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_epoch = TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "model_name = '/home/Tsung/pathology/weight/' + \"{}_MCnt_end2end_image&sideinfo_{}_loss_{}_bs_{}\".format(\n",
    "    current_time, target, train_loss, batch_size)\n",
    "\n",
    "cur_metric = 0\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "for i in range(0, epoch):\n",
    "    print('\\nEpoch: {}, batch: {}'.format(i, batch_size))\n",
    "    \n",
    "    # lr_scheduler.step()\n",
    "    adjust_learning_rate(optimizer, i, init_lr = init_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "    \n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "    train_history.append(train_logs)\n",
    "    valid_history.append(valid_logs)\n",
    "    \n",
    "    if cur_metric < valid_logs[metrics[0].__name__]:\n",
    "        cur_metric = valid_logs[metrics[0].__name__]\n",
    "        torch.save(model.state_dict(), model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "        print('Model saved!', model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "\n",
    "#     if cur_metric > valid_logs[metrics[0].__name__]:\n",
    "#         cur_metric = valid_logs[metrics[0].__name__]\n",
    "#         torch.save(model.state_dict(), model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "#         print('Model saved!', model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_history(history):\n",
    "    loss1 = []\n",
    "    metric1 = []\n",
    "    metric2 = []\n",
    "    for i in history:\n",
    "        loss1.append(i['cross entropy'])\n",
    "        metric1.append(i['accuracy'])\n",
    "    plt.figure()\n",
    "    p1 = plt.plot(loss1, label='cross entropy')\n",
    "    p2 = plt.plot(metric1, label='accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    img, info, gt = data\n",
    "    img = img.to(DEVICE)\n",
    "    info = info.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(img, info)\n",
    "    pred = torch.softmax(pred, dim = 1)\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    print(pred, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    train_cohorts,\n",
    "    train_counts,\n",
    "#     augmentation=get_training_augmentation(),\n",
    "#     preprocessing=get_preprocessing(),\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    test_cohorts,\n",
    "    test_counts,\n",
    "#     augmentation=get_validation_augmentation(),\n",
    "#     preprocessing=get_preprocessing(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(max_images = 200, d_model = 512, dropout=0.1)\n",
    "model.load_state_dict(torch.load('./weight/2020_09_09_16_07_mCount-reg-log-end2end_sampler_loss:model_arch_bs:cetest.h5'))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_list = []\n",
    "pred_list = []\n",
    "_temp = 0\n",
    "metric = Accuracy()\n",
    "for data in train_loader:\n",
    "    images, x2, labels = data\n",
    "    images = images.cuda()\n",
    "    x2 = x2.cuda()\n",
    "    labels = labels.cuda()\n",
    "    with torch.no_grad():\n",
    "        pred = model.forward(images, x2)\n",
    "    pred = torch.softmax(pred, axis = 1)\n",
    "    pred = torch.argmax(pred, axis = 1)\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = pred.squeeze()\n",
    "    pred_list.extend(pred)\n",
    "\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    gt_list.extend(labels)\n",
    "gt_list = np.array(gt_list)\n",
    "pred_list = np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,10))\n",
    "plt.plot(pred_list, color=\"g\")\n",
    "plt.plot(gt_list, color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt, pred in zip(gt_list, pred_list):\n",
    "    print(\"{:.4f}, {:.4f}, {:.4f}\".format(gt, pred, np.abs(gt-pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(gt_list, pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whole slide\n",
    "\n",
    "zStats density\n",
    "\n",
    "transformer pretrain\n",
    "svs 加入空間關係\n",
    "\n",
    "consine scheduler   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
