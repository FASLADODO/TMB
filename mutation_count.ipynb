{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'MultiheadAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7ae90985df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malbu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchsummary/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" torchsummary \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Some modules do the computation themselves using parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# or the parameters of children. Treat these as layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mLAYER_MODULES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mINPUT_SIZE_TYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m INPUT_DATA_TYPE = Optional[\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'MultiheadAttention'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil, glob, sys, math, cv2, re\n",
    "\n",
    "import albumentations as albu\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import normalizeStaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_record_folder = '/nfs/Shared/data/tcga/tumor'\n",
    "cohort_tumor_npy = sorted(os.listdir(tumor_record_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_images_dict = {i[:12]: [] for i in cohort_tumor_npy}    \n",
    "for npy in cohort_tumor_npy:\n",
    "    cohort_name = npy[:12]\n",
    "    image_names = np.load(os.path.join(tumor_record_folder, npy))\n",
    "    cohort_images_dict[cohort_name].extend(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數量500多，有些沒有images\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/coad_Mutation_Count.txt', delimiter=\"\\t\")\n",
    "cohort_count_dict = {row[1]: row[3] for index,row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cohorts, valid_cohorts = train_test_split(\n",
    "    list(cohort_images_dict.keys()), test_size=0.33, random_state=42)\n",
    "print(len(train_cohorts), len(valid_cohorts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_labels(cohorts, cohort_images_dict, cohort_count_dict):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    no_expand_labels = []\n",
    "    for cohort in cohorts:\n",
    "        images = cohort_images_dict[cohort]\n",
    "        counts = [cohort_count_dict[cohort]]*len(images)\n",
    "        all_images.extend(images)\n",
    "        all_labels.extend(counts)\n",
    "        no_expand_labels.append(cohort_count_dict[cohort])\n",
    "    return np.array(all_images), np.array(all_labels), np.array(no_expand_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt2percentile(counts, labels):\n",
    "    percentile = np.percentile(counts, [25,75])\n",
    "    print(percentile)\n",
    "    percentile_labels = np.zeros_like(labels)\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        _foo = (label < percentile)*1\n",
    "        if np.sum(_foo) == 0:\n",
    "            percentile_labels[idx] = 2\n",
    "        else:\n",
    "            for _temp, _i in enumerate(_foo):\n",
    "                if _i == 1:\n",
    "                    percentile_labels[idx] = _temp\n",
    "                    break\n",
    "#         print(label, percentile_labels[idx])\n",
    "    return percentile_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, train_counts = get_images_labels(train_cohorts, cohort_images_dict, cohort_count_dict)\n",
    "valid_images, valid_labels, valid_counts = get_images_labels(valid_cohorts, cohort_images_dict, cohort_count_dict)\n",
    "\n",
    "print(len(train_images), len(train_labels), len(train_counts), len(valid_images), len(valid_labels), len(valid_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = cvt2percentile(train_counts, train_labels)\n",
    "valid_labels = cvt2percentile(valid_counts, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(height = 256, width = 265, always_apply=True),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py, to_tensor     \n",
    "def to0_1(x, **kwargs):\n",
    "    return x/255\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "        albu.Lambda(image=to0_1, mask=to0_1),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    def __init__(self, image_array, label_array, augmentation=None, preprocessing=None):\n",
    "        self.image_array = image_array\n",
    "        self.label_array = label_array\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        fp = self.image_array[i]\n",
    "        \n",
    "        image = cv2.imread(fp)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         imgNorm= normalizeStaining.normalizeStaining(img = im_rgb)\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image = sample['image']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "        \n",
    "        label = self.label_array[i]\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "epoch = 60\n",
    "model_arch = 'resnet18'\n",
    "train_loss = 'ce'\n",
    "train_metric = 'fscore'\n",
    "use_sampler = True\n",
    "init_lr = 1e-4\n",
    "nclass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(in_features=512, out_features=nclass, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    augmentation=get_augmentation(), \n",
    "    preprocessing=get_preprocessing(),\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_images, \n",
    "    valid_labels, \n",
    "    preprocessing=get_preprocessing(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = utils.metrics.CrossEntropy()\n",
    "metrics = [\n",
    "    utils.metrics.Fscore(),\n",
    "    utils.metrics.Accuracy(),\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=init_lr),\n",
    "])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max=epoch, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "model_name = './weight/' + \"{}_MCnt-centile3-loss:{}_bs:{}\".format(\n",
    "    current_time, train_loss, bs)\n",
    "\n",
    "cur_metric = 0\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "for i in range(0, epoch):\n",
    "    print('\\nEpoch: {}, batch: {}'.format(i, bs))\n",
    "    \n",
    "    # lr_scheduler.step()\n",
    "    adjust_learning_rate(optimizer, i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "    \n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    train_history.append(train_logs)\n",
    "    valid_history.append(valid_logs)\n",
    "    \n",
    "    if cur_metric < valid_logs[metrics[0].__name__]:\n",
    "        cur_metric = valid_logs[metrics[0].__name__]\n",
    "        torch.save(model.state_dict(), model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "        print('Model saved!', model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "\n",
    "#     if cur_metric > valid_logs[metrics[0].__name__]:\n",
    "#         cur_metric = valid_logs[metrics[0].__name__]\n",
    "#         torch.save(model.state_dict(), model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")\n",
    "#         print('Model saved!', model_name+\"_epoch{}_{}:{:.4f}\".format(i,train_metric, cur_metric)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
